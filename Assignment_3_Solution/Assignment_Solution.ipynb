{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "#from utilities import *\n",
    "import torchfile as tr\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tr.load('../data.bin')\n",
    "test_data = tr.load('../test.bin')\n",
    "train_labels=tr.load('../labels.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape :(29160, 108, 108)\n",
      "Testing data shape :(29160, 108, 108)\n",
      "Training label data shape :(29160,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data shape :{}\".format(train_data.shape))\n",
    "print(\"Testing data shape :{}\".format(test_data.shape))\n",
    "print(\"Training label data shape :{}\".format(train_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train data to train and val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "test_x,valid_x,test_y,valid_y=train_test_split(train_data,train_labels,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image plot for data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=14\n",
    "img=train_data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa491ea2438>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping training data to (108*108,29160)\n",
    "train_data=test_x.reshape(test_x.shape[0],-1).T\n",
    "validation_data=valid_x.reshape(valid_x.shape[0],-1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping test data to shape (108*108,29160)\n",
    "test_data=test_data.reshape(test_data.shape[0],-1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping train labels to (29160,1)\n",
    "train_labels=test_y.reshape(test_y.shape[0],-1).T\n",
    "validation_labels=valid_y.reshape(valid_y.shape[0],-1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_acc=np.copy(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Lables to one hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels=np.squeeze(np.eye(6)[train_labels]).T\n",
    "#validation_labels=np.squeeze(np.eye(6)[validation_labels]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape after Reshape:(11664, 23328)\n",
      "Training label data shape after Reshape:(6, 23328)\n",
      "Testing data shape after Reshape:(11664, 29160)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data shape after Reshape:{}\".format(train_data.shape))\n",
    "print(\"Training label data shape after Reshape:{}\".format(train_labels.shape))\n",
    "print(\"Testing data shape after Reshape:{}\".format(test_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data/255.0\n",
    "train_data.shape\n",
    "validation_data = validation_data/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(layer_dims):\n",
    "    #np.random.seed(3)\n",
    "    #no_of_layers = len(layer_dims)-1\n",
    "    #parameters ={}\n",
    "    #for l in range(1, no_of_layers + 1):\n",
    "    #    parameters[\"W\"+str(l)]=np.random.randn(layer_dims[l],layer_dims[l-1]) * 0.01 #*np.sqrt(2./layer_dims[l-1])\n",
    "    #    parameters[\"b\"+str(l)]=np.zeros(shape=(layer_dims[l],1))\n",
    "    #parameters=np.load('parameters.npy',allow_pickle=True)\n",
    "    #print('parameters==',len(parameters))\n",
    "    # Load data (deserialize)\n",
    "    with open('parameters_15-11-2019_04-43-14_PM.pickle', 'rb') as handle:\n",
    "        parameters = pickle.load(handle)\n",
    "    #print('parameters==',len(parameters))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Mini Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation]\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X[:, k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X[:, (k+1)*mini_batch_size:]\n",
    "        mini_batch_Y = shuffled_Y[:, (k+1)*mini_batch_size:]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_forward(Z):\n",
    "    A=np.maximum(0,Z)\n",
    "    return A,Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(prev_der,saved_data):\n",
    "    this_layer_activation=saved_data  #saved_data=Z of this layer\n",
    "    d_this_layer=np.array(prev_der, copy=True)\n",
    "    #print(\"d of thsi layer===\",d_this_layer)\n",
    "    d_this_layer[this_layer_activation <=0 ]=0\n",
    "    #print(\"d of thsi layer===\",d_this_layer)\n",
    "    return d_this_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_forward(Z):\n",
    "    exp=np.exp(Z)\n",
    "    sum_exp=np.sum(exp,axis=0,keepdims=True)\n",
    "    softmax=exp/sum_exp\n",
    "    #Z -= np.max(Z)\n",
    "    #softmax = (np.exp(Z).T / np.sum(np.exp(Z),axis=1)).T\n",
    "    return softmax,Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_plus_activation(X,W,b,activation):\n",
    "    Z = np.dot(W,X)+b\n",
    "    \n",
    "    if activation==\"relu\":\n",
    "        A,activation_save_data=relu_forward(Z)\n",
    "    else:\n",
    "        A,activation_save_data=softmax_forward(Z)\n",
    "    linear_save_data=(X,W,b)\n",
    "    save_data=(linear_save_data,activation_save_data)\n",
    "    return A,save_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X,parameters):    \n",
    "    A=X\n",
    "    cache_data=[]\n",
    "    #no of layers\n",
    "    L=len(parameters)//2  #W and b so //2\n",
    "    for l in range(1,L):\n",
    "        A_prev=A\n",
    "        A ,relu_save_data= linear_plus_activation(A_prev,parameters[\"W\"+str(l)],parameters[\"b\"+str(l)],\"relu\")\n",
    "        cache_data.append(relu_save_data)\n",
    "    A_final ,softmax_save_data= linear_plus_activation(A,parameters[\"W\"+str(L)],parameters[\"b\"+str(L)],\"softmax\")\n",
    "    cache_data.append(softmax_save_data)\n",
    "    #print(\"A final : \",A_final.shape)\n",
    "    return A_final,cache_data\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_gradient_compute_linear(grad_from_top_layer,save_data,activation):\n",
    "    linear_saved_data,activation_saved_data=save_data\n",
    "    total_images=grad_from_top_layer.shape[1]\n",
    "    if activation==\"relu\":\n",
    "        grad_from_top=relu_backward(grad_from_top_layer,activation_saved_data)\n",
    "    prev_activation,W,b=linear_saved_data\n",
    "    #print(\"top grad==\",grad_from_top)\n",
    "    dW=np.dot(grad_from_top,prev_activation.T)/total_images\n",
    "    db=np.sum(grad_from_top,axis=1,keepdims=True)/total_images\n",
    "    d_pass_to_prev_layer=np.dot(W.T,grad_from_top)\n",
    "    #print(\"dw==\",dW)\n",
    "    return d_pass_to_prev_layer,dW,db\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_gradient_compute_linear_with_regularization(grad_from_top_layer,save_data,activation,lambd):\n",
    "    linear_saved_data,activation_saved_data=save_data\n",
    "    total_images=grad_from_top_layer.shape[1]\n",
    "    if activation==\"relu\":\n",
    "        grad_from_top=relu_backward(grad_from_top_layer,activation_saved_data)\n",
    "    prev_activation,W,b=linear_saved_data\n",
    "    #print(\"top grad==\",grad_from_top)\n",
    "    dW=np.dot(grad_from_top,prev_activation.T)/total_images+lambd*W/total_images\n",
    "    db=np.sum(grad_from_top,axis=1,keepdims=True)/total_images\n",
    "    d_pass_to_prev_layer=np.dot(W.T,grad_from_top)\n",
    "    #print(\"dw==\",dW)\n",
    "    return d_pass_to_prev_layer,dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(output_from_softmax,Y,saved_data):\n",
    "    gradients={}\n",
    "    prev_derivate=output_from_softmax-Y\n",
    "    \n",
    "    number_of_layers=len(saved_data)\n",
    "    total_images=Y.shape[1]\n",
    "    final_layer_saved=saved_data[number_of_layers-1]\n",
    "    linear,activation=final_layer_saved\n",
    "    gradients['dW'+str(number_of_layers)]=np.dot(prev_derivate,linear[0].T)/total_images\n",
    "    gradients['db'+str(number_of_layers)]=np.sum(prev_derivate,axis=1,keepdims=True)/total_images\n",
    "    prev_derivate=np.dot(linear[1].T,prev_derivate)\n",
    "    for l in reversed(range(number_of_layers-1)):\n",
    "        current_layer_save_data=saved_data[l]\n",
    "        prev_derivate,dW,db=backward_gradient_compute_linear(prev_derivate,current_layer_save_data,\"relu\")\n",
    "        gradients['dW'+str(l+1)]=dW\n",
    "        gradients['db'+str(l+1)]=db\n",
    "        #print(\"gradient shapes of W{0}:{1} and b{2}:{3}\".format(l,dW.shape,l,db.shape))\n",
    "    #print(\"dW1===\",gradients[\"dW1\"])\n",
    "    return gradients\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop_with_regularization(output_from_softmax,Y,saved_data,lambd):\n",
    "    gradients={}\n",
    "    prev_derivate=output_from_softmax-Y\n",
    "    \n",
    "    number_of_layers=len(saved_data)\n",
    "    total_images=Y.shape[1]\n",
    "    final_layer_saved=saved_data[number_of_layers-1]\n",
    "    linear,activation=final_layer_saved\n",
    "    gradients['dW'+str(number_of_layers)]=np.dot(prev_derivate,linear[0].T)/total_images+lambd*linear[1]/total_images\n",
    "    gradients['db'+str(number_of_layers)]=np.sum(prev_derivate,axis=1,keepdims=True)/total_images\n",
    "    prev_derivate=np.dot(linear[1].T,prev_derivate)\n",
    "    for l in reversed(range(number_of_layers-1)):\n",
    "        current_layer_save_data=saved_data[l]\n",
    "        prev_derivate,dW,db=backward_gradient_compute_linear_with_regularization(prev_derivate,current_layer_save_data,\"relu\",lambd)\n",
    "        gradients['dW'+str(l+1)]=dW\n",
    "        gradients['db'+str(l+1)]=db\n",
    "        #print(\"gradient shapes of W{0}:{1} and b{2}:{3}\".format(l,dW.shape,l,db.shape))\n",
    "    #print(\"dW1===\",gradients[\"dW1\"])\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(y_hat,y):\n",
    "    #print(\"shape of y_hat {} and y {}\".format(y_hat.shape,y.shape))\n",
    "    total_images=y.shape[1]\n",
    "    #cost=-np.sum(y*np.log(y_hat)+(1-y)*np.log(1-y_hat))/total_images\n",
    "    L_sum = np.sum(np.multiply(y, np.log(y_hat)))\n",
    "    L = -(1./total_images) * L_sum\n",
    "\n",
    "    L = np.squeeze(L) \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost_with_regularization\n",
    "\n",
    "def compute_cost_with_regularization(y_hat, y, parameters, lambd):\n",
    "    total_images = y.shape[1]\n",
    "    number_of_layers = len(parameters)//2\n",
    "        \n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    \n",
    "    cross_entropy_cost = cost(y_hat, y) # This gives you the cross-entropy part of the cost\n",
    "    \n",
    "    ### START CODE HERE ### (approx. 1 line)\n",
    "    L2_regularization_cost = lambd * (np.sum(np.square(W1))+np.sum(np.square(W2))+np.sum(np.square(W3)))/(2*total_images)\n",
    "    ### END CODER HERE ###\n",
    "    \n",
    "    total_cost = cross_entropy_cost + L2_regularization_cost\n",
    "    \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update parameters with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(parameters,gradients,learning_rate):\n",
    "    number_of_layers=len(parameters)//2  # sice W and B in 1 params so /2\n",
    "    #print(\"prev weight \",parameters[\"W1\"])\n",
    "    #print(\"prev grad \",gradients[\"dW1\"])\n",
    "    for l in range(number_of_layers):\n",
    "        parameters[\"W\"+str(l+1)]-=learning_rate*gradients[\"dW\"+str(l+1)]\n",
    "        parameters[\"b\"+str(l+1)]-=learning_rate*gradients[\"db\"+str(l+1)]\n",
    "    #print(\"new weight \",parameters[\"W1\"])\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update parameters with gradient descent with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_with_adam(parameters,gradients,learning_rate,beta,beta2):\n",
    "    number_of_layers=len(parameters)//2  # sice W and B in 1 params so /2\n",
    "    #print(\"prev weight \",parameters[\"W1\"])\n",
    "    #print(\"prev grad \",gradients[\"dW1\"])\n",
    "    # Initialize velocity\n",
    "    v={}\n",
    "    s={}\n",
    "    epsilon=1e-8\n",
    "    for l in range(number_of_layers):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        v[\"dW\" + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)\n",
    "        s[\"dW\" + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n",
    "        s[\"db\" + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)\n",
    "\n",
    "    for l in range(number_of_layers):\n",
    "        # compute velocities\n",
    "        v[\"dW\" + str(l+1)] = beta*v[\"dW\" + str(l+1)] + (1-beta)*gradients['dW' + str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta*v[\"db\" + str(l+1)] + (1-beta)*gradients['db' + str(l+1)]\n",
    "        \n",
    "        # Moving average of the squared gradients. \n",
    "        \n",
    "        s[\"dW\" + str(l+1)] = beta2*s[\"dW\" + str(l+1)] + (1-beta2)*(gradients[\"dW\" + str(l+1)]**2)\n",
    "        s[\"db\" + str(l+1)] = beta2*s[\"db\" + str(l+1)] + (1-beta2)*(gradients[\"db\" + str(l+1)]**2)\n",
    "        \n",
    "        # update parameters\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*v[\"dW\" + str(l+1)]\\\n",
    "                                                                    /(s[\"dW\" + str(l+1)]**0.5+epsilon)\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*v[\"db\" + str(l+1)]\\\n",
    "                                                                    /(s[\"db\" + str(l+1)]**0.5+epsilon)\n",
    "    #print(\"new weight \",parameters[\"W1\"])\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_with_momentum(parameters,gradients,learning_rate,beta):\n",
    "    number_of_layers=len(parameters)//2  # sice W and B in 1 params so /2\n",
    "    #print(\"prev weight \",parameters[\"W1\"])\n",
    "    #print(\"prev grad \",gradients[\"dW1\"])\n",
    "    # Initialize velocity\n",
    "    v={}\n",
    "    s={}\n",
    "    epsilon=1e-8\n",
    "    for l in range(number_of_layers):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        v[\"dW\" + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)\n",
    "        \n",
    "    for l in range(number_of_layers):\n",
    "        # compute velocities\n",
    "        v[\"dW\" + str(l+1)] = beta*v[\"dW\" + str(l+1)] + (1-beta)*gradients['dW' + str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta*v[\"db\" + str(l+1)] + (1-beta)*gradients['db' + str(l+1)]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # update parameters\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*v[\"dW\" + str(l+1)]\n",
    "                                                                    \n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*v[\"db\" + str(l+1)]\n",
    "                                                                    \n",
    "    #print(\"new weight \",parameters[\"W1\"])\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "#layers_dims = [11664,32,10, 6] #  3-layer model\n",
    "#layers_dims = [11664,10,6]\n",
    "layers_dims = [11664,150,60,32,10, 6]\n",
    "learning_rate = 1e-3\n",
    "num_iterations = 50\n",
    "L2_regularizer_lambd=0.01\n",
    "momentum_beta=0.9\n",
    "momentum_beta2=0.999\n",
    "mini_batch_size=32\n",
    "learning_rate_decay=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model(X,Y,layer_dims,learning_rate , num_iterations,learning_rate_decay):\n",
    "    costs = [] \n",
    "    seed = 0\n",
    "    \n",
    "    parameters=initialization(layer_dims)\n",
    "    iterations_per_epoch = max(X.shape[1] / mini_batch_size, 1)\n",
    "    #print(\"para   \",parameters)\n",
    "    for i in range(0,num_iterations):\n",
    "        seed = seed + 1\n",
    "        train_accuracy=[]\n",
    "        valid_accuracy = []\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "        for indx,minibatch in enumerate(minibatches):\n",
    "            \n",
    "            # Select a minibatch\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "            #forward propogation\n",
    "            y_hat,cache_data = forward_prop(minibatch_X,parameters)\n",
    "            #iter_cost=cost(y_hat,minibatch_Y) #non-regularized cost\n",
    "            #print(\"y_hat :\",y_hat[:,0])\n",
    "            #print(\"y :\",minibatch_Y[:,0])\n",
    "            iter_cost = compute_cost_with_regularization(y_hat,minibatch_Y,parameters,L2_regularizer_lambd)\n",
    "            #gradients=backward_prop(y_hat,minibatch_Y,cache_data)\n",
    "            gradients = backward_prop_with_regularization(y_hat,minibatch_Y,cache_data,L2_regularizer_lambd)\n",
    "            parameters=update_with_momentum(parameters,gradients,learning_rate,momentum_beta)\n",
    "            if indx % 100 == 0:\n",
    "                train_acc = accuracy(y_hat,minibatch_Y)\n",
    "                valid_acc = predict(validation_data,validation_labels,parameters)\n",
    "                #print(\"train acc:\",train_acc)\n",
    "                train_accuracy.append(train_acc)\n",
    "                valid_accuracy.append(valid_acc)\n",
    "                #print(\"Cost after epoch {0} and batch : {1}: {2}\".format(i+1,indx+1, np.squeeze(iter_cost)))\n",
    "                costs.append(iter_cost)\n",
    "            \n",
    "            \n",
    "            \n",
    "        print(\"Train accuracy after epoch {} : {}\".format(i+1,np.mean(train_accuracy)))\n",
    "        print(\"Validation accuracy after epoch {} : {}\".format(i+1,np.mean(valid_accuracy)))\n",
    "        print(\"Mean Cost over all batches after epoch {0} : {1}\".format(i+1,np.mean(np.squeeze(costs))))\n",
    "    # Decay learning rate\n",
    "    \n",
    "        learning_rate = learning_rate * (1/1+learning_rate_decay*i)\n",
    "\n",
    "        print(\"\\nreduced learning rate to : {}\\n\".format(learning_rate))\n",
    "        print(\"\\n-------------------------------------\\n\")\n",
    "    time = datetime.now().strftime(\"%d-%m-%Y_%I-%M-%S_%p\")\n",
    "    with open('parameters_{0}.pickle'.format(time), 'wb') as handle:\n",
    "        pickle.dump(parameters, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return parameters,costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_old(X,Y,layer_dims,learning_rate , num_iterations):\n",
    "    costs = [] \n",
    "    seed = 0\n",
    "    parameters=initialization(layer_dims)\n",
    "    #print(\"para   \",parameters)\n",
    "    for i in range(0,num_iterations):\n",
    "        \n",
    "        y_hat,cache_data = forward_prop(X,parameters)\n",
    "        #iter_cost=cost(y_hat,minibatch_Y) #non-regularized cost\n",
    "        iter_cost = compute_cost_with_regularization(y_hat,Y,parameters,L2_regularizer_lambd)\n",
    "\n",
    "        if i % 1 == 0:\n",
    "            print(\"\\n Cost after iteration {}: {} \\n\".format(i+1, np.squeeze(iter_cost)))\n",
    "            costs.append(iter_cost)\n",
    "\n",
    "        #gradients=backward_prop(y_hat,minibatch_Y,cache_data)\n",
    "        gradients = backward_prop_with_regularization(y_hat,Y,cache_data,L2_regularizer_lambd)\n",
    "        parameters=update(parameters,gradients,learning_rate)\n",
    "\n",
    "        \n",
    "        if i%1000==0:\n",
    "            time = datetime.now().strftime(\"%d-%m-%Y_%I-%M-%S_%p\")\n",
    "            with open('parameters_{0}.pickle'.format(time), 'wb') as handle:\n",
    "                pickle.dump(parameters, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return parameters,costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot cost for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost(costs):\n",
    "    # plot the cost\n",
    "    fig = plt.figure()\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundred)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    #plt.show()\n",
    "    plt.savefig('cost_2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \n",
    "    total_images = y.shape[1]\n",
    "    p = np.zeros((1,total_images), dtype = np.int)\n",
    "    \n",
    "    # Forward propagation\n",
    "    y_hat,cache_data = forward_prop(X,parameters)\n",
    "    \n",
    "    prediction=np.argmax(y_hat,axis=0)\n",
    "    prediction=np.squeeze(prediction.reshape(y_hat.shape[1],1))\n",
    "    actual_label=np.squeeze(y)\n",
    "    accuracy = sum(prediction == actual_label)/(float(len(actual_label)))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    \n",
    "    total_images = y.shape[1]\n",
    "    p = np.zeros((1,total_images), dtype = np.int)\n",
    "    \n",
    "    \n",
    "    prediction=np.argmax(y_hat,axis=0)\n",
    "    prediction=np.squeeze(prediction.reshape(y_hat.shape[1],1))\n",
    "    reference=np.argmax(y,axis=0)\n",
    "    reference=np.squeeze(reference.reshape(y.shape[1],1))\n",
    "    \n",
    "    \n",
    "    accuracy = sum(prediction == reference)/(float(len(reference)))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy after epoch 1 : 0.71875\n",
      "Validation accuracy after epoch 1 : 0.5344435871056241\n",
      "Mean Cost over all batches after epoch 1 : 0.7980884341697353\n",
      "\n",
      "reduced learning rate to : 0.001\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 2 : 0.73828125\n",
      "Validation accuracy after epoch 2 : 0.539673353909465\n",
      "Mean Cost over all batches after epoch 2 : 0.7937645004251757\n",
      "\n",
      "reduced learning rate to : 0.001001\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 3 : 0.734375\n",
      "Validation accuracy after epoch 3 : 0.5399734224965707\n",
      "Mean Cost over all batches after epoch 3 : 0.8004197210892056\n",
      "\n",
      "reduced learning rate to : 0.001003002\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 4 : 0.65234375\n",
      "Validation accuracy after epoch 4 : 0.5401663237311385\n",
      "Mean Cost over all batches after epoch 4 : 0.8315391440892647\n",
      "\n",
      "reduced learning rate to : 0.001006011006\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 5 : 0.68359375\n",
      "Validation accuracy after epoch 5 : 0.53963048696845\n",
      "Mean Cost over all batches after epoch 5 : 0.8412336877577946\n",
      "\n",
      "reduced learning rate to : 0.001010035050024\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 6 : 0.7734375\n",
      "Validation accuracy after epoch 6 : 0.5393732853223594\n",
      "Mean Cost over all batches after epoch 6 : 0.8264641230179745\n",
      "\n",
      "reduced learning rate to : 0.0010150852252741199\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 7 : 0.7265625\n",
      "Validation accuracy after epoch 7 : 0.5411093964334706\n",
      "Mean Cost over all batches after epoch 7 : 0.8233081561674374\n",
      "\n",
      "reduced learning rate to : 0.0010211757366257645\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 8 : 0.76171875\n",
      "Validation accuracy after epoch 8 : 0.541045096021948\n",
      "Mean Cost over all batches after epoch 8 : 0.8156121662608635\n",
      "\n",
      "reduced learning rate to : 0.0010283239667821448\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 9 : 0.73828125\n",
      "Validation accuracy after epoch 9 : 0.540616426611797\n",
      "Mean Cost over all batches after epoch 9 : 0.8068509660566304\n",
      "\n",
      "reduced learning rate to : 0.001036550558516402\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 10 : 0.71875\n",
      "Validation accuracy after epoch 10 : 0.5409164951989025\n",
      "Mean Cost over all batches after epoch 10 : 0.8081460185319136\n",
      "\n",
      "reduced learning rate to : 0.0010458795135430494\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 11 : 0.765625\n",
      "Validation accuracy after epoch 11 : 0.541366598079561\n",
      "Mean Cost over all batches after epoch 11 : 0.8017712775551021\n",
      "\n",
      "reduced learning rate to : 0.00105633830867848\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 12 : 0.71875\n",
      "Validation accuracy after epoch 12 : 0.5411951303155007\n",
      "Mean Cost over all batches after epoch 12 : 0.8072914473406287\n",
      "\n",
      "reduced learning rate to : 0.001067958030073943\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 13 : 0.69921875\n",
      "Validation accuracy after epoch 13 : 0.5413022976680384\n",
      "Mean Cost over all batches after epoch 13 : 0.8099517765130249\n",
      "\n",
      "reduced learning rate to : 0.0010807735264348303\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 14 : 0.73828125\n",
      "Validation accuracy after epoch 14 : 0.5418810013717421\n",
      "Mean Cost over all batches after epoch 14 : 0.8084025648385669\n",
      "\n",
      "reduced learning rate to : 0.001094823582278483\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 15 : 0.72265625\n",
      "Validation accuracy after epoch 15 : 0.5419881687242798\n",
      "Mean Cost over all batches after epoch 15 : 0.8067640946362696\n",
      "\n",
      "reduced learning rate to : 0.0011101511124303818\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 16 : 0.74609375\n",
      "Validation accuracy after epoch 16 : 0.5409164951989025\n",
      "Mean Cost over all batches after epoch 16 : 0.804667042102732\n",
      "\n",
      "reduced learning rate to : 0.0011268033791168375\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 17 : 0.74609375\n",
      "Validation accuracy after epoch 17 : 0.5407235939643347\n",
      "Mean Cost over all batches after epoch 17 : 0.8052977066420848\n",
      "\n",
      "reduced learning rate to : 0.001144832233182707\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 18 : 0.71484375\n",
      "Validation accuracy after epoch 18 : 0.5413022976680384\n",
      "Mean Cost over all batches after epoch 18 : 0.8036367837498842\n",
      "\n",
      "reduced learning rate to : 0.0011642943811468129\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 19 : 0.73046875\n",
      "Validation accuracy after epoch 19 : 0.5412594307270233\n",
      "Mean Cost over all batches after epoch 19 : 0.8027540744932599\n",
      "\n",
      "reduced learning rate to : 0.0011852516800074555\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 20 : 0.69921875\n",
      "Validation accuracy after epoch 20 : 0.5412808641975309\n",
      "Mean Cost over all batches after epoch 20 : 0.8045345839654672\n",
      "\n",
      "reduced learning rate to : 0.0012077714619275971\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 21 : 0.76953125\n",
      "Validation accuracy after epoch 21 : 0.5412594307270233\n",
      "Mean Cost over all batches after epoch 21 : 0.7991172433874845\n",
      "\n",
      "reduced learning rate to : 0.001231926891166149\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 22 : 0.75390625\n",
      "Validation accuracy after epoch 22 : 0.5417738340192044\n",
      "Mean Cost over all batches after epoch 22 : 0.7975209651330221\n",
      "\n",
      "reduced learning rate to : 0.001257797355880638\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 23 : 0.68359375\n",
      "Validation accuracy after epoch 23 : 0.5407664609053499\n",
      "Mean Cost over all batches after epoch 23 : 0.7996819193680894\n",
      "\n",
      "reduced learning rate to : 0.0012854688977100121\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 24 : 0.71484375\n",
      "Validation accuracy after epoch 24 : 0.5409593621399177\n",
      "Mean Cost over all batches after epoch 24 : 0.8006723692045498\n",
      "\n",
      "reduced learning rate to : 0.0013150346823573424\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 25 : 0.703125\n",
      "Validation accuracy after epoch 25 : 0.5411736968449932\n",
      "Mean Cost over all batches after epoch 25 : 0.8015858633290119\n",
      "\n",
      "reduced learning rate to : 0.0013465955147339186\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 26 : 0.7109375\n",
      "Validation accuracy after epoch 26 : 0.5420310356652949\n",
      "Mean Cost over all batches after epoch 26 : 0.8026918599520487\n",
      "\n",
      "reduced learning rate to : 0.0013802604026022665\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 27 : 0.765625\n",
      "Validation accuracy after epoch 27 : 0.5404878257887518\n",
      "Mean Cost over all batches after epoch 27 : 0.7987334383493008\n",
      "\n",
      "reduced learning rate to : 0.0014161471730699256\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 28 : 0.69921875\n",
      "Validation accuracy after epoch 28 : 0.5416881001371743\n",
      "Mean Cost over all batches after epoch 28 : 0.8000112068869988\n",
      "\n",
      "reduced learning rate to : 0.0014543831467428135\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 29 : 0.70703125\n",
      "Validation accuracy after epoch 29 : 0.5407878943758573\n",
      "Mean Cost over all batches after epoch 29 : 0.7993564108672075\n",
      "\n",
      "reduced learning rate to : 0.0014951058748516122\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 30 : 0.71875\n",
      "Validation accuracy after epoch 30 : 0.5416452331961592\n",
      "Mean Cost over all batches after epoch 30 : 0.7987893934289685\n",
      "\n",
      "reduced learning rate to : 0.0015384639452223087\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 31 : 0.77734375\n",
      "Validation accuracy after epoch 31 : 0.5408736282578875\n",
      "Mean Cost over all batches after epoch 31 : 0.7968734276058776\n",
      "\n",
      "reduced learning rate to : 0.001584617863578978\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 32 : 0.7578125\n",
      "Validation accuracy after epoch 32 : 0.5410879629629629\n",
      "Mean Cost over all batches after epoch 32 : 0.795912629144421\n",
      "\n",
      "reduced learning rate to : 0.0016337410173499262\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 33 : 0.73828125\n",
      "Validation accuracy after epoch 33 : 0.5411308299039781\n",
      "Mean Cost over all batches after epoch 33 : 0.7942192879999812\n",
      "\n",
      "reduced learning rate to : 0.0016860207299051239\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy after epoch 34 : 0.71875\n",
      "Validation accuracy after epoch 34 : 0.5411093964334706\n",
      "Mean Cost over all batches after epoch 34 : 0.7925783127623207\n",
      "\n",
      "reduced learning rate to : 0.0017416594139919928\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 35 : 0.703125\n",
      "Validation accuracy after epoch 35 : 0.541838134430727\n",
      "Mean Cost over all batches after epoch 35 : 0.7933369682278891\n",
      "\n",
      "reduced learning rate to : 0.0018008758340677207\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Train accuracy after epoch 36 : 0.74609375\n",
      "Validation accuracy after epoch 36 : 0.5413880315500685\n",
      "Mean Cost over all batches after epoch 36 : 0.7912896350115332\n",
      "\n",
      "reduced learning rate to : 0.0018639064882600908\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paramerters,costs = model(train_data,train_labels,layers_dims,learning_rate,num_iterations,learning_rate_decay)\n",
    "#plot cost\n",
    "plot_cost(costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training Accuracy: 0.6636231138545954\n"
     ]
    }
   ],
   "source": [
    "# Train accuracy\n",
    "predict(train_data,train_labels_acc,paramerters,\"Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.5044581618655692\n"
     ]
    }
   ],
   "source": [
    "# Validation accuracy\n",
    "predict(validation_data,validation_labels,paramerters,\"Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CSV for kaggle upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_submission_csv(X,parameters):\n",
    "    # Forward propagation\n",
    "    y_hat,cache_data = forward_prop(X,parameters)\n",
    "    \n",
    "    prediction=np.argmax(y_hat,axis=0)\n",
    "    prediction=np.squeeze(prediction.reshape(y_hat.shape[1],1))\n",
    "    df = pd.DataFrame(data=prediction,columns=[\"label\"])\n",
    "    df = df.rename_axis('id').reset_index()\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = convert_submission_csv(test_data,paramerters)\n",
    "df.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
