{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "#from utilities import *\n",
    "import torchfile as tr\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tr.load('../data.bin')\n",
    "test_data = tr.load('../test.bin')\n",
    "train_labels=tr.load('../labels.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape :(29160, 108, 108)\n",
      "Testing data shape :(29160, 108, 108)\n",
      "Training label data shape :(29160,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data shape :{}\".format(train_data.shape))\n",
    "print(\"Testing data shape :{}\".format(test_data.shape))\n",
    "print(\"Training label data shape :{}\".format(train_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train data to train and val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "test_x,valid_x,test_y,valid_y=train_test_split(train_data,train_labels,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image plot for data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=14\n",
    "img=train_data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa1d0828748>"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping training data to (108*108,29160)\n",
    "train_data=test_x.reshape(test_x.shape[0],-1).T\n",
    "validation_data=valid_x.reshape(valid_x.shape[0],-1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping test data to shape (108*108,29160)\n",
    "test_data=test_data.reshape(test_data.shape[0],-1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping train labels to (29160,1)\n",
    "train_labels=test_y.reshape(test_y.shape[0],-1).T\n",
    "validation_labels=valid_y.reshape(valid_y.shape[0],-1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_acc=np.copy(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Lables to one hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels=np.squeeze(np.eye(6)[train_labels]).T\n",
    "#validation_labels=np.squeeze(np.eye(6)[validation_labels]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape after Reshape:(11664, 23328)\n",
      "Training label data shape after Reshape:(6, 23328)\n",
      "Testing data shape after Reshape:(11664, 29160)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data shape after Reshape:{}\".format(train_data.shape))\n",
    "print(\"Training label data shape after Reshape:{}\".format(train_labels.shape))\n",
    "print(\"Testing data shape after Reshape:{}\".format(test_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(layer_dims):\n",
    "    #np.random.seed(3)\n",
    "    #no_of_layers = len(layer_dims)-1\n",
    "    #parameters ={}\n",
    "    #for l in range(1, no_of_layers + 1):\n",
    "    #    parameters[\"W\"+str(l)]=np.random.randn(layer_dims[l],layer_dims[l-1]) * 0.01 #*np.sqrt(2./layer_dims[l-1])\n",
    "    #    parameters[\"b\"+str(l)]=np.zeros(shape=(layer_dims[l],1))\n",
    "    #parameters=np.load('parameters.npy',allow_pickle=True)\n",
    "    #print('parameters==',len(parameters))\n",
    "    # Load data (deserialize)\n",
    "    with open('parameters_15-11-2019_09-38-04_AM.pickle', 'rb') as handle:\n",
    "        parameters = pickle.load(handle)\n",
    "    #print('parameters==',len(parameters))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Mini Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation]\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X[:, k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X[:, (k+1)*mini_batch_size:]\n",
    "        mini_batch_Y = shuffled_Y[:, (k+1)*mini_batch_size:]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_forward(Z):\n",
    "    A=np.maximum(0,Z)\n",
    "    return A,Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(prev_der,saved_data):\n",
    "    this_layer_activation=saved_data  #saved_data=Z of this layer\n",
    "    d_this_layer=np.array(prev_der, copy=True)\n",
    "    #print(\"d of thsi layer===\",d_this_layer)\n",
    "    d_this_layer[this_layer_activation <=0 ]=0\n",
    "    #print(\"d of thsi layer===\",d_this_layer)\n",
    "    return d_this_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_forward(Z):\n",
    "    exp=np.exp(Z)\n",
    "    sum_exp=np.sum(exp,axis=0,keepdims=True)\n",
    "    softmax=exp/sum_exp\n",
    "    #Z -= np.max(Z)\n",
    "    #softmax = (np.exp(Z).T / np.sum(np.exp(Z),axis=1)).T\n",
    "    return softmax,Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_plus_activation(X,W,b,activation):\n",
    "    Z = np.dot(W,X)+b\n",
    "    \n",
    "    if activation==\"relu\":\n",
    "        A,activation_save_data=relu_forward(Z)\n",
    "    else:\n",
    "        A,activation_save_data=softmax_forward(Z)\n",
    "    linear_save_data=(X,W,b)\n",
    "    save_data=(linear_save_data,activation_save_data)\n",
    "    return A,save_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X,parameters):    \n",
    "    A=X\n",
    "    cache_data=[]\n",
    "    #no of layers\n",
    "    L=len(parameters)//2  #W and b so //2\n",
    "    for l in range(1,L):\n",
    "        A_prev=A\n",
    "        A ,relu_save_data= linear_plus_activation(A_prev,parameters[\"W\"+str(l)],parameters[\"b\"+str(l)],\"relu\")\n",
    "        cache_data.append(relu_save_data)\n",
    "    A_final ,softmax_save_data= linear_plus_activation(A,parameters[\"W\"+str(L)],parameters[\"b\"+str(L)],\"softmax\")\n",
    "    cache_data.append(softmax_save_data)\n",
    "    return A_final,cache_data\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_gradient_compute_linear(grad_from_top_layer,save_data,activation):\n",
    "    linear_saved_data,activation_saved_data=save_data\n",
    "    total_images=grad_from_top_layer.shape[1]\n",
    "    if activation==\"relu\":\n",
    "        grad_from_top=relu_backward(grad_from_top_layer,activation_saved_data)\n",
    "    prev_activation,W,b=linear_saved_data\n",
    "    #print(\"top grad==\",grad_from_top)\n",
    "    dW=np.dot(grad_from_top,prev_activation.T)/total_images\n",
    "    db=np.sum(grad_from_top,axis=1,keepdims=True)/total_images\n",
    "    d_pass_to_prev_layer=np.dot(W.T,grad_from_top)\n",
    "    #print(\"dw==\",dW)\n",
    "    return d_pass_to_prev_layer,dW,db\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_gradient_compute_linear_with_regularization(grad_from_top_layer,save_data,activation,lambd):\n",
    "    linear_saved_data,activation_saved_data=save_data\n",
    "    total_images=grad_from_top_layer.shape[1]\n",
    "    if activation==\"relu\":\n",
    "        grad_from_top=relu_backward(grad_from_top_layer,activation_saved_data)\n",
    "    prev_activation,W,b=linear_saved_data\n",
    "    #print(\"top grad==\",grad_from_top)\n",
    "    dW=np.dot(grad_from_top,prev_activation.T)/total_images+lambd*W/total_images\n",
    "    db=np.sum(grad_from_top,axis=1,keepdims=True)/total_images\n",
    "    d_pass_to_prev_layer=np.dot(W.T,grad_from_top)\n",
    "    #print(\"dw==\",dW)\n",
    "    return d_pass_to_prev_layer,dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(output_from_softmax,Y,saved_data):\n",
    "    gradients={}\n",
    "    prev_derivate=output_from_softmax-Y\n",
    "    \n",
    "    number_of_layers=len(saved_data)\n",
    "    total_images=Y.shape[1]\n",
    "    final_layer_saved=saved_data[number_of_layers-1]\n",
    "    linear,activation=final_layer_saved\n",
    "    gradients['dW'+str(number_of_layers)]=np.dot(prev_derivate,linear[0].T)/total_images\n",
    "    gradients['db'+str(number_of_layers)]=np.sum(prev_derivate,axis=1,keepdims=True)/total_images\n",
    "    prev_derivate=np.dot(linear[1].T,prev_derivate)\n",
    "    for l in reversed(range(number_of_layers-1)):\n",
    "        current_layer_save_data=saved_data[l]\n",
    "        prev_derivate,dW,db=backward_gradient_compute_linear(prev_derivate,current_layer_save_data,\"relu\")\n",
    "        gradients['dW'+str(l+1)]=dW\n",
    "        gradients['db'+str(l+1)]=db\n",
    "        #print(\"gradient shapes of W{0}:{1} and b{2}:{3}\".format(l,dW.shape,l,db.shape))\n",
    "    #print(\"dW1===\",gradients[\"dW1\"])\n",
    "    return gradients\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop_with_regularization(output_from_softmax,Y,saved_data,lambd):\n",
    "    gradients={}\n",
    "    prev_derivate=output_from_softmax-Y\n",
    "    \n",
    "    number_of_layers=len(saved_data)\n",
    "    total_images=Y.shape[1]\n",
    "    final_layer_saved=saved_data[number_of_layers-1]\n",
    "    linear,activation=final_layer_saved\n",
    "    gradients['dW'+str(number_of_layers)]=np.dot(prev_derivate,linear[0].T)/total_images+lambd*linear[1]/total_images\n",
    "    gradients['db'+str(number_of_layers)]=np.sum(prev_derivate,axis=1,keepdims=True)/total_images\n",
    "    prev_derivate=np.dot(linear[1].T,prev_derivate)\n",
    "    for l in reversed(range(number_of_layers-1)):\n",
    "        current_layer_save_data=saved_data[l]\n",
    "        prev_derivate,dW,db=backward_gradient_compute_linear_with_regularization(prev_derivate,current_layer_save_data,\"relu\",lambd)\n",
    "        gradients['dW'+str(l+1)]=dW\n",
    "        gradients['db'+str(l+1)]=db\n",
    "        #print(\"gradient shapes of W{0}:{1} and b{2}:{3}\".format(l,dW.shape,l,db.shape))\n",
    "    #print(\"dW1===\",gradients[\"dW1\"])\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(y_hat,y):\n",
    "    #print(\"shape of y_hat {} and y {}\".format(y_hat.shape,y.shape))\n",
    "    total_images=y.shape[1]\n",
    "    #cost=-np.sum(y*np.log(y_hat)+(1-y)*np.log(1-y_hat))/total_images\n",
    "    L_sum = np.sum(np.multiply(y, np.log(y_hat)))\n",
    "    L = -(1./total_images) * L_sum\n",
    "\n",
    "    L = np.squeeze(L) \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost_with_regularization\n",
    "\n",
    "def compute_cost_with_regularization(y_hat, y, parameters, lambd):\n",
    "    total_images = y.shape[1]\n",
    "    number_of_layers = len(parameters)//2\n",
    "        \n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    \n",
    "    cross_entropy_cost = cost(y_hat, y) # This gives you the cross-entropy part of the cost\n",
    "    \n",
    "    ### START CODE HERE ### (approx. 1 line)\n",
    "    L2_regularization_cost = lambd * (np.sum(np.square(W1))+np.sum(np.square(W2))+np.sum(np.square(W3)))/(2*total_images)\n",
    "    ### END CODER HERE ###\n",
    "    \n",
    "    total_cost = cross_entropy_cost + L2_regularization_cost\n",
    "    \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update parameters with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(parameters,gradients,learning_rate):\n",
    "    number_of_layers=len(parameters)//2  # sice W and B in 1 params so /2\n",
    "    #print(\"prev weight \",parameters[\"W1\"])\n",
    "    #print(\"prev grad \",gradients[\"dW1\"])\n",
    "    for l in range(number_of_layers):\n",
    "        parameters[\"W\"+str(l+1)]-=learning_rate*gradients[\"dW\"+str(l+1)]\n",
    "        parameters[\"b\"+str(l+1)]-=learning_rate*gradients[\"db\"+str(l+1)]\n",
    "    #print(\"new weight \",parameters[\"W1\"])\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update parameters with gradient descent with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_with_momentum(parameters,gradients,learning_rate,beta,beta2):\n",
    "    number_of_layers=len(parameters)//2  # sice W and B in 1 params so /2\n",
    "    #print(\"prev weight \",parameters[\"W1\"])\n",
    "    #print(\"prev grad \",gradients[\"dW1\"])\n",
    "    # Initialize velocity\n",
    "    v={}\n",
    "    s={}\n",
    "    epsilon=1e-8\n",
    "    for l in range(number_of_layers):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        v[\"dW\" + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)\n",
    "        s[\"dW\" + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n",
    "        s[\"db\" + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)\n",
    "\n",
    "    for l in range(number_of_layers):\n",
    "        # compute velocities\n",
    "        v[\"dW\" + str(l+1)] = beta*v[\"dW\" + str(l+1)] + (1-beta)*gradients['dW' + str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta*v[\"db\" + str(l+1)] + (1-beta)*gradients['db' + str(l+1)]\n",
    "        \n",
    "        # Moving average of the squared gradients. \n",
    "        \n",
    "        s[\"dW\" + str(l+1)] = beta2*s[\"dW\" + str(l+1)] + (1-beta2)*(gradients[\"dW\" + str(l+1)]**2)\n",
    "        s[\"db\" + str(l+1)] = beta2*s[\"db\" + str(l+1)] + (1-beta2)*(gradients[\"db\" + str(l+1)]**2)\n",
    "        \n",
    "        # update parameters\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*v[\"dW\" + str(l+1)]\\\n",
    "                                                                    /(s[\"dW\" + str(l+1)]**0.5+epsilon)\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*v[\"db\" + str(l+1)]\\\n",
    "                                                                    /(s[\"db\" + str(l+1)]**0.5+epsilon)\n",
    "    #print(\"new weight \",parameters[\"W1\"])\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "layers_dims = [11664,32, 10, 6] #  3-layer model\n",
    "#layers_dims = [11664,10,6]\n",
    "learning_rate = 1e-3\n",
    "num_iterations = 100\n",
    "L2_regularizer_lambd=0.1\n",
    "momentum_beta=0.999\n",
    "momentum_beta2=0.9\n",
    "mini_batch_size=64\n",
    "learning_rate_decay=0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model(X,Y,layer_dims,learning_rate , num_iterations,learning_rate_decay):\n",
    "    costs = [] \n",
    "    seed = 0\n",
    "    parameters=initialization(layer_dims)\n",
    "    iterations_per_epoch = max(X.shape[1] / mini_batch_size, 1)\n",
    "    #print(\"para   \",parameters)\n",
    "    for i in range(0,num_iterations):\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "        for indx,minibatch in enumerate(minibatches):\n",
    "            # Select a minibatch\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "            #forward propogation\n",
    "            y_hat,cache_data = forward_prop(minibatch_X,parameters)\n",
    "            #iter_cost=cost(y_hat,minibatch_Y) #non-regularized cost\n",
    "            iter_cost = compute_cost_with_regularization(y_hat,minibatch_Y,parameters,L2_regularizer_lambd)\n",
    "                \n",
    "            \n",
    "            if indx % 100 == 0:\n",
    "                #train_acc = (y_hat == minibatch_Y).mean()\n",
    "                \n",
    "                \n",
    "                #print(\"train accuracy after epoch {} : {}\".format(train_acc,i+1))\n",
    "                print(\"Cost after epoch {0} and batch : {1}: {2}\".format(i+1,indx+1, np.squeeze(iter_cost)))\n",
    "                costs.append(iter_cost)\n",
    "            \n",
    "            #gradients=backward_prop(y_hat,minibatch_Y,cache_data)\n",
    "            gradients = backward_prop_with_regularization(y_hat,minibatch_Y,cache_data,L2_regularizer_lambd)\n",
    "            parameters=update_with_momentum(parameters,gradients,learning_rate,momentum_beta,momentum_beta2)\n",
    "\n",
    "        if i % 1 == 0:\n",
    "                print(\"\\n Cost after iteration {}: {} \\n\".format(i+1, np.squeeze(iter_cost)))\n",
    "                costs.append(iter_cost)\n",
    "        if i % 10 == 0:\n",
    "                # Decay learning rate\n",
    "                learning_rate *= learning_rate_decay\n",
    "                print(\"\\nreduced learning rate to : {}\\n\".format(learning_rate))\n",
    "        if i%10==0:\n",
    "            time = datetime.now().strftime(\"%d-%m-%Y_%I-%M-%S_%p\")\n",
    "            with open('parameters_{0}.pickle'.format(time), 'wb') as handle:\n",
    "                pickle.dump(parameters, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return parameters,costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_old(X,Y,layer_dims,learning_rate , num_iterations):\n",
    "    costs = [] \n",
    "    seed = 0\n",
    "    parameters=initialization(layer_dims)\n",
    "    #print(\"para   \",parameters)\n",
    "    for i in range(0,num_iterations):\n",
    "        \n",
    "        y_hat,cache_data = forward_prop(X,parameters)\n",
    "        #iter_cost=cost(y_hat,minibatch_Y) #non-regularized cost\n",
    "        iter_cost = compute_cost_with_regularization(y_hat,Y,parameters,L2_regularizer_lambd)\n",
    "\n",
    "        if i % 1 == 0:\n",
    "            print(\"\\n Cost after iteration {}: {} \\n\".format(i+1, np.squeeze(iter_cost)))\n",
    "            costs.append(iter_cost)\n",
    "\n",
    "        #gradients=backward_prop(y_hat,minibatch_Y,cache_data)\n",
    "        gradients = backward_prop_with_regularization(y_hat,Y,cache_data,L2_regularizer_lambd)\n",
    "        parameters=update(parameters,gradients,learning_rate)\n",
    "\n",
    "        \n",
    "        if i%1000==0:\n",
    "            time = datetime.now().strftime(\"%d-%m-%Y_%I-%M-%S_%p\")\n",
    "            with open('parameters_{0}.pickle'.format(time), 'wb') as handle:\n",
    "                pickle.dump(parameters, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return parameters,costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot cost for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost(costs):\n",
    "    # plot the cost\n",
    "    fig = plt.figure()\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundred)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    #plt.show()\n",
    "    plt.savefig('cost_2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters,acc_type):\n",
    "    \n",
    "    total_images = y.shape[1]\n",
    "    p = np.zeros((1,total_images), dtype = np.int)\n",
    "    \n",
    "    # Forward propagation\n",
    "    y_hat,cache_data = forward_prop(X,parameters)\n",
    "    \n",
    "    prediction=np.argmax(y_hat,axis=0)\n",
    "    prediction=np.squeeze(prediction.reshape(y_hat.shape[1],1))\n",
    "    actual_label=np.squeeze(y)\n",
    "    accuracy = sum(prediction == actual_label)/(float(len(actual_label)))\n",
    "    \n",
    "    print(\" {} Accuracy: {}\".format(acc_type,accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 1 and batch : 1: 1.4249556939319268\n",
      "Cost after epoch 1 and batch : 101: 1.3112659877404875\n",
      "Cost after epoch 1 and batch : 201: 1.458409043725561\n",
      "Cost after epoch 1 and batch : 301: 1.393150372987975\n",
      "\n",
      " Cost after iteration 1: 1.5590965660229477 \n",
      "\n",
      "\n",
      "reduced learning rate to : 0.00095\n",
      "\n",
      "Cost after epoch 2 and batch : 1: 1.3860955142988618\n",
      "Cost after epoch 2 and batch : 101: 1.3583611730032983\n",
      "Cost after epoch 2 and batch : 201: 1.3040898928164943\n",
      "Cost after epoch 2 and batch : 301: 1.4631521277613242\n",
      "\n",
      " Cost after iteration 2: 1.356877679527289 \n",
      "\n",
      "Cost after epoch 3 and batch : 1: 1.4153762896313427\n",
      "Cost after epoch 3 and batch : 101: 1.546334333338941\n",
      "Cost after epoch 3 and batch : 201: 1.3520618877310213\n",
      "Cost after epoch 3 and batch : 301: 1.3640505390430475\n",
      "\n",
      " Cost after iteration 3: 1.427454933443222 \n",
      "\n",
      "Cost after epoch 4 and batch : 1: 1.2805965342052823\n",
      "Cost after epoch 4 and batch : 101: 1.44927530987814\n",
      "Cost after epoch 4 and batch : 201: 1.3405178850105877\n",
      "Cost after epoch 4 and batch : 301: 1.5224021957135003\n",
      "\n",
      " Cost after iteration 4: 1.4667038841809839 \n",
      "\n",
      "Cost after epoch 5 and batch : 1: 1.2919156421479423\n",
      "Cost after epoch 5 and batch : 101: 1.2558224715099904\n",
      "Cost after epoch 5 and batch : 201: 1.3539233552696828\n",
      "Cost after epoch 5 and batch : 301: 1.4927489813896895\n",
      "\n",
      " Cost after iteration 5: 1.2469457993200221 \n",
      "\n",
      "Cost after epoch 6 and batch : 1: 1.2430737164030963\n",
      "Cost after epoch 6 and batch : 101: 1.309890169812148\n",
      "Cost after epoch 6 and batch : 201: 1.4473279967178836\n",
      "Cost after epoch 6 and batch : 301: 1.2446579685581547\n",
      "\n",
      " Cost after iteration 6: 1.463136417576348 \n",
      "\n",
      "Cost after epoch 7 and batch : 1: 1.414036781669329\n",
      "Cost after epoch 7 and batch : 101: 1.3756814354332596\n",
      "Cost after epoch 7 and batch : 201: 1.3100424660144576\n",
      "Cost after epoch 7 and batch : 301: 1.3613217255277958\n",
      "\n",
      " Cost after iteration 7: 1.448594483100531 \n",
      "\n",
      "Cost after epoch 8 and batch : 1: 1.1939156976592908\n",
      "Cost after epoch 8 and batch : 101: 1.3085569521977236\n",
      "Cost after epoch 8 and batch : 201: 1.373254458728034\n",
      "Cost after epoch 8 and batch : 301: 1.2866670634904078\n",
      "\n",
      " Cost after iteration 8: 1.4381077712664738 \n",
      "\n",
      "Cost after epoch 9 and batch : 1: 1.4148484969311335\n",
      "Cost after epoch 9 and batch : 101: 1.3135146055654185\n",
      "Cost after epoch 9 and batch : 201: 1.354473425953679\n",
      "Cost after epoch 9 and batch : 301: 1.3730739909598548\n",
      "\n",
      " Cost after iteration 9: 1.2809056023096268 \n",
      "\n",
      "Cost after epoch 10 and batch : 1: 1.3731211180126646\n",
      "Cost after epoch 10 and batch : 101: 1.3835051017059146\n",
      "Cost after epoch 10 and batch : 201: 1.3471867112128164\n",
      "Cost after epoch 10 and batch : 301: 1.4027816610756951\n",
      "\n",
      " Cost after iteration 10: 1.2125738972984246 \n",
      "\n",
      "Cost after epoch 11 and batch : 1: 1.334263030301281\n",
      "Cost after epoch 11 and batch : 101: 1.320775618197221\n",
      "Cost after epoch 11 and batch : 201: 1.4770034288294573\n",
      "Cost after epoch 11 and batch : 301: 1.276960486818669\n",
      "\n",
      " Cost after iteration 11: 1.4827042641235395 \n",
      "\n",
      "\n",
      "reduced learning rate to : 0.0009025\n",
      "\n",
      "Cost after epoch 12 and batch : 1: 1.3357544631824145\n",
      "Cost after epoch 12 and batch : 101: 1.420646930218244\n",
      "Cost after epoch 12 and batch : 201: 1.4952767371598306\n",
      "Cost after epoch 12 and batch : 301: 1.6165551052538087\n",
      "\n",
      " Cost after iteration 12: 1.228702420607241 \n",
      "\n",
      "Cost after epoch 13 and batch : 1: 1.328586226204309\n",
      "Cost after epoch 13 and batch : 101: 1.3374545729772613\n",
      "Cost after epoch 13 and batch : 201: 1.46594941266599\n",
      "Cost after epoch 13 and batch : 301: 1.289400014511246\n",
      "\n",
      " Cost after iteration 13: 1.2778114719308409 \n",
      "\n",
      "Cost after epoch 14 and batch : 1: 1.4513893242117697\n",
      "Cost after epoch 14 and batch : 101: 1.337361100633863\n",
      "Cost after epoch 14 and batch : 201: 1.3330404121520412\n",
      "Cost after epoch 14 and batch : 301: 1.4074167528076806\n",
      "\n",
      " Cost after iteration 14: 1.4222731407432827 \n",
      "\n",
      "Cost after epoch 15 and batch : 1: 1.453710956476235\n",
      "Cost after epoch 15 and batch : 101: 1.4625763236405167\n",
      "Cost after epoch 15 and batch : 201: 1.4705139119615833\n",
      "Cost after epoch 15 and batch : 301: 1.4272843134468411\n",
      "\n",
      " Cost after iteration 15: 1.316751651851391 \n",
      "\n",
      "Cost after epoch 16 and batch : 1: 1.3323842586376875\n",
      "Cost after epoch 16 and batch : 101: 1.4215769459097465\n",
      "Cost after epoch 16 and batch : 201: 1.3275139823315159\n",
      "Cost after epoch 16 and batch : 301: 1.6013858949987483\n",
      "\n",
      " Cost after iteration 16: 1.368117191622633 \n",
      "\n",
      "Cost after epoch 17 and batch : 1: 1.3023289797454038\n",
      "Cost after epoch 17 and batch : 101: 1.3189340360182509\n",
      "Cost after epoch 17 and batch : 201: 1.4885347731457257\n",
      "Cost after epoch 17 and batch : 301: 1.3005485265269705\n",
      "\n",
      " Cost after iteration 17: 1.5328565252213389 \n",
      "\n",
      "Cost after epoch 18 and batch : 1: 1.522609497427172\n",
      "Cost after epoch 18 and batch : 101: 1.3819549734090322\n",
      "Cost after epoch 18 and batch : 201: 1.3760143764297081\n",
      "Cost after epoch 18 and batch : 301: 1.2612594419315344\n",
      "\n",
      " Cost after iteration 18: 1.0696423799775228 \n",
      "\n",
      "Cost after epoch 19 and batch : 1: 1.3780494086374364\n",
      "Cost after epoch 19 and batch : 101: 1.330184465390993\n",
      "Cost after epoch 19 and batch : 201: 1.4459405842010944\n",
      "Cost after epoch 19 and batch : 301: 1.3938637457896732\n",
      "\n",
      " Cost after iteration 19: 1.3824279362268208 \n",
      "\n",
      "Cost after epoch 20 and batch : 1: 1.4444047231417014\n",
      "Cost after epoch 20 and batch : 101: 1.3633819852376907\n",
      "Cost after epoch 20 and batch : 201: 1.3129853718908107\n",
      "Cost after epoch 20 and batch : 301: 1.1994707136248068\n",
      "\n",
      " Cost after iteration 20: 1.4711678538261241 \n",
      "\n",
      "Cost after epoch 21 and batch : 1: 1.1988628949224054\n",
      "Cost after epoch 21 and batch : 101: 1.2851192710389407\n",
      "Cost after epoch 21 and batch : 201: 1.2308592306284671\n",
      "Cost after epoch 21 and batch : 301: 1.444340583031236\n",
      "\n",
      " Cost after iteration 21: 1.6163924238162433 \n",
      "\n",
      "\n",
      "reduced learning rate to : 0.000857375\n",
      "\n",
      "Cost after epoch 22 and batch : 1: 1.3465322695351303\n",
      "Cost after epoch 22 and batch : 101: 1.3911602826950116\n",
      "Cost after epoch 22 and batch : 201: 1.3982989794452876\n",
      "Cost after epoch 22 and batch : 301: 1.3061493198406713\n",
      "\n",
      " Cost after iteration 22: 1.3818316281672636 \n",
      "\n",
      "Cost after epoch 23 and batch : 1: 1.4060880193699656\n",
      "Cost after epoch 23 and batch : 101: 1.551476326408457\n",
      "Cost after epoch 23 and batch : 201: 1.3307723650695147\n",
      "Cost after epoch 23 and batch : 301: 1.4633646999857093\n",
      "\n",
      " Cost after iteration 23: 1.2366352649630423 \n",
      "\n",
      "Cost after epoch 24 and batch : 1: 1.47833514494994\n",
      "Cost after epoch 24 and batch : 101: 1.3578747435940235\n",
      "Cost after epoch 24 and batch : 201: 1.5087487727230426\n",
      "Cost after epoch 24 and batch : 301: 1.495727393380167\n",
      "\n",
      " Cost after iteration 24: 1.439074725738945 \n",
      "\n",
      "Cost after epoch 25 and batch : 1: 1.3484161425571186\n",
      "Cost after epoch 25 and batch : 101: 1.3568926498781402\n",
      "Cost after epoch 25 and batch : 201: 1.326361323135255\n",
      "Cost after epoch 25 and batch : 301: 1.2568731917489175\n",
      "\n",
      " Cost after iteration 25: 1.6344107276304305 \n",
      "\n",
      "Cost after epoch 26 and batch : 1: 1.4835559786386119\n",
      "Cost after epoch 26 and batch : 101: 1.4534499150896565\n",
      "Cost after epoch 26 and batch : 201: 1.4248850502464667\n",
      "Cost after epoch 26 and batch : 301: 1.3765657494064478\n",
      "\n",
      " Cost after iteration 26: 1.3661050157765977 \n",
      "\n",
      "Cost after epoch 27 and batch : 1: 1.3447330302572615\n",
      "Cost after epoch 27 and batch : 101: 1.3275968751579696\n",
      "Cost after epoch 27 and batch : 201: 1.2864973862951792\n",
      "Cost after epoch 27 and batch : 301: 1.2507498045438157\n",
      "\n",
      " Cost after iteration 27: 1.189172895965749 \n",
      "\n",
      "Cost after epoch 28 and batch : 1: 1.175719398441569\n",
      "Cost after epoch 28 and batch : 101: 1.397126445781254\n",
      "Cost after epoch 28 and batch : 201: 1.4511885406084213\n",
      "Cost after epoch 28 and batch : 301: 1.4039023097085663\n",
      "\n",
      " Cost after iteration 28: 1.3040599913680784 \n",
      "\n",
      "Cost after epoch 29 and batch : 1: 1.2932001488877352\n",
      "Cost after epoch 29 and batch : 101: 1.5188562305023723\n",
      "Cost after epoch 29 and batch : 201: 1.3712761948959697\n",
      "Cost after epoch 29 and batch : 301: 1.2497311886366151\n",
      "\n",
      " Cost after iteration 29: 1.3564737438190617 \n",
      "\n",
      "Cost after epoch 30 and batch : 1: 1.4092797683337122\n",
      "Cost after epoch 30 and batch : 101: 1.4300686846667137\n",
      "Cost after epoch 30 and batch : 201: 1.487545443873452\n",
      "Cost after epoch 30 and batch : 301: 1.2279935119635612\n",
      "\n",
      " Cost after iteration 30: 1.4567565997470509 \n",
      "\n",
      "Cost after epoch 31 and batch : 1: 1.4132303421244106\n",
      "Cost after epoch 31 and batch : 101: 1.3832014633879703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 31 and batch : 201: 1.3004659852905351\n",
      "Cost after epoch 31 and batch : 301: 1.3230617065862502\n",
      "\n",
      " Cost after iteration 31: 1.569048195405235 \n",
      "\n",
      "\n",
      "reduced learning rate to : 0.0008145062499999999\n",
      "\n",
      "Cost after epoch 32 and batch : 1: 1.3187497239725787\n",
      "Cost after epoch 32 and batch : 101: 1.3723079195544694\n",
      "Cost after epoch 32 and batch : 201: 1.280642307805151\n",
      "Cost after epoch 32 and batch : 301: 1.2952630467152253\n",
      "\n",
      " Cost after iteration 32: 1.2724103713720303 \n",
      "\n",
      "Cost after epoch 33 and batch : 1: 1.2742185396489698\n",
      "Cost after epoch 33 and batch : 101: 1.4977781661534844\n",
      "Cost after epoch 33 and batch : 201: 1.321946756496465\n",
      "Cost after epoch 33 and batch : 301: 1.1444901438907473\n",
      "\n",
      " Cost after iteration 33: 1.5299882411711145 \n",
      "\n",
      "Cost after epoch 34 and batch : 1: 1.4771616039197295\n",
      "Cost after epoch 34 and batch : 101: 1.2740501178096026\n",
      "Cost after epoch 34 and batch : 201: 1.355215458276007\n",
      "Cost after epoch 34 and batch : 301: 1.3579458202479822\n",
      "\n",
      " Cost after iteration 34: 1.282538506058671 \n",
      "\n",
      "Cost after epoch 35 and batch : 1: 1.423852265585743\n",
      "Cost after epoch 35 and batch : 101: 1.3519525275029745\n",
      "Cost after epoch 35 and batch : 201: 1.3224379881077588\n",
      "Cost after epoch 35 and batch : 301: 1.3754632131733378\n",
      "\n",
      " Cost after iteration 35: 1.3061222020434469 \n",
      "\n",
      "Cost after epoch 36 and batch : 1: 1.3960398836795522\n",
      "Cost after epoch 36 and batch : 101: 1.268631928345396\n",
      "Cost after epoch 36 and batch : 201: 1.3050350943202766\n",
      "Cost after epoch 36 and batch : 301: 1.4884196515816082\n",
      "\n",
      " Cost after iteration 36: 1.5125453190668365 \n",
      "\n",
      "Cost after epoch 37 and batch : 1: 1.3316573140407104\n",
      "Cost after epoch 37 and batch : 101: 1.1822240358428\n",
      "Cost after epoch 37 and batch : 201: 1.2935964542295284\n",
      "Cost after epoch 37 and batch : 301: 1.2954972071910555\n",
      "\n",
      " Cost after iteration 37: 1.2853373817189517 \n",
      "\n",
      "Cost after epoch 38 and batch : 1: 1.3758496425656659\n",
      "Cost after epoch 38 and batch : 101: 1.2034002496891858\n",
      "Cost after epoch 38 and batch : 201: 1.4042467555349605\n",
      "Cost after epoch 38 and batch : 301: 1.2959155560908262\n",
      "\n",
      " Cost after iteration 38: 1.560004013097471 \n",
      "\n",
      "Cost after epoch 39 and batch : 1: 1.3775882448629375\n",
      "Cost after epoch 39 and batch : 101: 1.4122874015231215\n",
      "Cost after epoch 39 and batch : 201: 1.3791233281355717\n",
      "Cost after epoch 39 and batch : 301: 1.3484089930849703\n",
      "\n",
      " Cost after iteration 39: 1.0862098566074891 \n",
      "\n",
      "Cost after epoch 40 and batch : 1: 1.2727347949019603\n",
      "Cost after epoch 40 and batch : 101: 1.1616786984456644\n",
      "Cost after epoch 40 and batch : 201: 1.4145413990733444\n",
      "Cost after epoch 40 and batch : 301: 1.417482771870081\n",
      "\n",
      " Cost after iteration 40: 1.341512766754955 \n",
      "\n",
      "Cost after epoch 41 and batch : 1: 1.410473886904447\n",
      "Cost after epoch 41 and batch : 101: 1.506907877884812\n",
      "Cost after epoch 41 and batch : 201: 1.3510513496193386\n",
      "Cost after epoch 41 and batch : 301: 1.496068231163856\n",
      "\n",
      " Cost after iteration 41: 1.2356940253524757 \n",
      "\n",
      "\n",
      "reduced learning rate to : 0.0007737809374999998\n",
      "\n",
      "Cost after epoch 42 and batch : 1: 1.3264951241757341\n",
      "Cost after epoch 42 and batch : 101: 1.1740575937088429\n",
      "Cost after epoch 42 and batch : 201: 1.2998246519919805\n",
      "Cost after epoch 42 and batch : 301: 1.4799943663701391\n",
      "\n",
      " Cost after iteration 42: 1.555987260057425 \n",
      "\n",
      "Cost after epoch 43 and batch : 1: 1.3890542167394522\n",
      "Cost after epoch 43 and batch : 101: 1.4377588724072312\n",
      "Cost after epoch 43 and batch : 201: 1.3296654185321872\n",
      "Cost after epoch 43 and batch : 301: 1.3985902528133158\n",
      "\n",
      " Cost after iteration 43: 1.3380443530540918 \n",
      "\n",
      "Cost after epoch 44 and batch : 1: 1.473582054568486\n",
      "Cost after epoch 44 and batch : 101: 1.4927338596997046\n",
      "Cost after epoch 44 and batch : 201: 1.517993781543837\n",
      "Cost after epoch 44 and batch : 301: 1.3347964767795581\n",
      "\n",
      " Cost after iteration 44: 1.3066584729576038 \n",
      "\n",
      "Cost after epoch 45 and batch : 1: 1.246055046924504\n",
      "Cost after epoch 45 and batch : 101: 1.3912395719445336\n",
      "Cost after epoch 45 and batch : 201: 1.3072784891179408\n",
      "Cost after epoch 45 and batch : 301: 1.2394988615861426\n",
      "\n",
      " Cost after iteration 45: 1.4351735800361434 \n",
      "\n",
      "Cost after epoch 46 and batch : 1: 1.2742314438104063\n",
      "Cost after epoch 46 and batch : 101: 1.5264795528409634\n",
      "Cost after epoch 46 and batch : 201: 1.35860014231175\n",
      "Cost after epoch 46 and batch : 301: 1.4505668723959162\n",
      "\n",
      " Cost after iteration 46: 1.2900953216642932 \n",
      "\n",
      "Cost after epoch 47 and batch : 1: 1.4728249451846098\n",
      "Cost after epoch 47 and batch : 101: 1.406882387404013\n",
      "Cost after epoch 47 and batch : 201: 1.5563978612912257\n",
      "Cost after epoch 47 and batch : 301: 1.2291792582851633\n",
      "\n",
      " Cost after iteration 47: 1.2823162652609217 \n",
      "\n",
      "Cost after epoch 48 and batch : 1: 1.3910506959732152\n",
      "Cost after epoch 48 and batch : 101: 1.2814400135296296\n",
      "Cost after epoch 48 and batch : 201: 1.2966920742470842\n",
      "Cost after epoch 48 and batch : 301: 1.3302874380006688\n",
      "\n",
      " Cost after iteration 48: 1.4356901254865917 \n",
      "\n",
      "Cost after epoch 49 and batch : 1: 1.362907987643365\n",
      "Cost after epoch 49 and batch : 101: 1.365881296046993\n",
      "Cost after epoch 49 and batch : 201: 1.4661223700435373\n",
      "Cost after epoch 49 and batch : 301: 1.1804589856776506\n",
      "\n",
      " Cost after iteration 49: 1.2830708929048167 \n",
      "\n",
      "Cost after epoch 50 and batch : 1: 1.2925500300864605\n",
      "Cost after epoch 50 and batch : 101: 1.1767020836533268\n",
      "Cost after epoch 50 and batch : 201: 1.2730008889750037\n",
      "Cost after epoch 50 and batch : 301: 1.2750322283105158\n",
      "\n",
      " Cost after iteration 50: 1.423692691968481 \n",
      "\n",
      "Cost after epoch 51 and batch : 1: 1.412154185905092\n",
      "Cost after epoch 51 and batch : 101: 1.30996919771958\n",
      "Cost after epoch 51 and batch : 201: 1.3355359150185313\n"
     ]
    }
   ],
   "source": [
    "paramerters,costs = model(train_data,train_labels,layers_dims,learning_rate,num_iterations,learning_rate_decay)\n",
    "#plot cost\n",
    "plot_cost(costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training Accuracy: 0.4742798353909465\n"
     ]
    }
   ],
   "source": [
    "# Train accuracy\n",
    "predict(train_data,train_labels_acc,paramerters,\"Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Accuracy: 0.4094650205761317\n"
     ]
    }
   ],
   "source": [
    "# Validation accuracy\n",
    "predict(validation_data,validation_labels,paramerters,\"Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CSV for kaggle upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_submission_csv(X,parameters):\n",
    "    # Forward propagation\n",
    "    y_hat,cache_data = forward_prop(X,parameters)\n",
    "    \n",
    "    prediction=np.argmax(y_hat,axis=0)\n",
    "    prediction=np.squeeze(prediction.reshape(y_hat.shape[1],1))\n",
    "    df = pd.DataFrame(data=prediction,columns=[\"label\"])\n",
    "    df = df.rename_axis('id').reset_index()\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = convert_submission_csv(test_data,paramerters)\n",
    "df.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
